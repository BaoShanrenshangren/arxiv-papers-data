{
  "id": "2512.17917v1",
  "title": "KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction",
  "authors": [
    "Aomufei Yuan, Zhiming Wang, Ruijie Miao, Dayu Wang, Yuxuan Tian, Zihan Wang, Yebo Peng, Yuhan Wu, Bairen Yi, Xin Liu, Tong Yang"
  ],
  "abstract": "arXiv:2512.17917v1 Announce Type: cross \nAbstract: As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging \"less important\" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.",
  "url": "https://arxiv.org/abs/2512.17917",
  "html_url": "https://arxiv.org/html/2512.17917v1",
  "html_content": "",
  "preview_text": "arXiv:2512.17917v1 Announce Type: cross \nAbstract: As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging \"less important\" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "KV cache compression",
    "sketch algorithm",
    "reversible compression",
    "memory efficiency",
    "LLM deployment"
  ],
  "one_line_summary": "KVReviver是一种基于草图算法的可逆KV缓存压缩方法，旨在减少大型语言模型部署中的内存需求，同时保持推理准确性。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:47:49.917329",
  "updated_at": "2025-12-23T15:47:49.917343"
}