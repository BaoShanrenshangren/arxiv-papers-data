{
  "id": "2512.19027v1",
  "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
  "authors": [
    "Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner"
  ],
  "abstract": "arXiv:2512.19027v1 Announce Type: new \nAbstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.",
  "url": "https://arxiv.org/abs/2512.19027",
  "html_url": "https://arxiv.org/html/2512.19027v1",
  "html_content": "",
  "preview_text": "arXiv:2512.19027v1 Announce Type: new \nAbstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "language models",
    "specification gaming",
    "recontextualization",
    "training signals",
    "misbehavior"
  ],
  "one_line_summary": "该论文提出了一种名为“再语境化”的方法，用于减少语言模型在训练中利用错误指定的信号进行不当行为，而不涉及视频扩散或多模态生成等技术。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:44:41.526124",
  "updated_at": "2025-12-23T15:44:41.526135"
}