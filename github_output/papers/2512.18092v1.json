{
  "id": "2512.18092v1",
  "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
  "authors": [
    "Ge Yan (Lily), Tuomas Oikarinen (Lily),  Tsui-Wei (Lily),  Weng"
  ],
  "abstract": "arXiv:2512.18092v1 Announce Type: new \nAbstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.",
  "url": "https://arxiv.org/abs/2512.18092",
  "html_url": "https://arxiv.org/html/2512.18092v1",
  "html_content": "",
  "preview_text": "arXiv:2512.18092v1 Announce Type: new \nAbstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.",
  "is_relevant": false,
  "relevance_score": 0.0,
  "extracted_keywords": [
    "neuron identification",
    "mechanistic interpretability",
    "faithfulness",
    "stability",
    "generalization bounds",
    "bootstrap ensemble"
  ],
  "one_line_summary": "该论文提出了一种基于理论分析的神经元识别方法，旨在保证解释的忠实性和稳定性，以提升机制可解释性的可信度。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:44:27.662212",
  "updated_at": "2025-12-23T15:44:27.662226"
}