{
  "id": "2512.18605v1",
  "title": "Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction",
  "authors": [
    "Qinglin Zeng, Jing Yang, Keze Wang"
  ],
  "abstract": "arXiv:2512.18605v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.\n  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.",
  "url": "https://arxiv.org/abs/2512.18605",
  "html_url": "https://arxiv.org/html/2512.18605v1",
  "html_content": "",
  "preview_text": "arXiv:2512.18605v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.\n  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "LLM",
    "reasoning",
    "self-correction",
    "efficiency",
    "confidence"
  ],
  "one_line_summary": "该论文提出了一种基于反思置信度的推理框架，通过在线自我纠正来提高大型语言模型在数学推理任务中的效率和准确性。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:44:34.680095",
  "updated_at": "2025-12-23T15:44:34.680106"
}