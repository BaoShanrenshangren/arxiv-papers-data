{
  "id": "2512.01550v1",
  "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
  "authors": [
    "Fei Liu",
    "Shichao Xie",
    "Minghua Luo",
    "Zedong Chu",
    "Junjun Hu",
    "Xiaolong Wu",
    "Mu Xu"
  ],
  "abstract": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.",
  "url": "https://arxiv.org/abs/2512.01550v1",
  "html_url": "https://arxiv.org/html/2512.01550v1",
  "html_content": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework.\nOur approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM’s structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\nI\nIntroduction\nEmbodied navigation, a cornerstone challenge in artificial intelligence, has recently witnessed remarkable progress driven by the advent of Vision-Language Models (VLMs)\n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n. These models endow agents with the ability to perceive, interpret instructions, and operate in complex environments. Despite these advances, a significant performance gap persists in long-horizon tasks, where agents frequently fail to maintain course, comprehend observations, or make consistently correct decisions. This gap stems from two primary limitations: (1) a planning and memory deficit, as deployable VLMs often have limited context windows and planning capabilities, causing them to get ”lost” in the navigation environment\n[\n6\n,\n7\n,\n8\n]\n; and (2) a lack of predictive foresight, as current models are fundamentally reactive and cannot anticipate future environmental states to guide their actions proactively\n[\n9\n,\n10\n,\n11\n]\n.\nExisting research has pursued these challenges on separate fronts. One trajectory enhances VLM reasoning through curated datasets and Chain-of-Thought (CoT) prompting\n[\n12\n,\n13\n]\n. The other develops world models to predict future states, informing action planning\n[\n14\n,\n15\n]\n. However, a critical oversight is the disconnection between these paradigms. A VLM-centric agent can suffer from semantic hallucinations, where its plan disconnects from visual reality, while a world model without language guidance can experience semantic drift, its predictions becoming untethered from the instructional goal.\nWe posit that VLM planning and predictive foresight should not be separate but unified and mutually reinforcing within a single VLM\n[\n16\n]\n. To this end, we introduce NavForesee as in Figure\nLABEL:fig:navforesee\n, a unified model that integrates multi-modal understanding with world model generation. Our approach is inspired by human navigation, which is not a continuous, low-level process but a hierarchical one centered on milestones. Humans typically navigate by heading towards a sequence of meaningful landmarks, largely ignoring the minutiae of the path between them. We argue that an artificial agent should do the same. NavForesee adopts this strategy by operating through two synergistic functions: (1) Hierarchical Language planning. As a planner, NavForesee generates a high-level plan by summarizing the navigation task into completed sub-instructions, identifying the current sub-instruction, and formulating the next steps as semantic action ”trunks.” This grounds the agent’s planning in the overall instruction. (2) Dual-Horizon Predictive Foresight. As a world model, NavForesee ”imagines” the future on two timescales. For long-term guidance, it predicts the key visual features of the environment at the completion of the current sub-instruction—effectively envisioning the next milestone. For short-term execution, it forecasts immediate future features to enhance local awareness, enabling robust obstacle avoidance and better understanding of environmental dynamics.\nInspired by latent-space world models\n[\n17\n,\n18\n,\n19\n,\n15\n]\n, this prediction deliberately avoids computationally expensive pixel-level generation. Instead, NavForesee forecasts a compact set of high-level features—depth, DINOv2, and SAM features—that capture essential geometric and semantic information as in DreamVLA. The predicted features are fed to an action policy module which is simply an MLP to generate continuous waypoints and flags for arriving or not. By tightly coupling hierarchical planning with dual-horizon predictive foresight, NavForesee generates coherent, goal-oriented actions, guided by both a long-term vision of its milestones and an immediate awareness of its surroundings.\nWe conducted extensive experiments on the R2R-CE\n[\n20\n]\nand RxR-CE\n[\n21\n]\nbenchmarks. Training exclusively on the publicly available R2R-CE and RxR-CE datasets, NavForesee demonstrates highly competitive performance, achieving a Success Rate (SR) of 66.2% and an Oracle Success Rate (OSR) of 78.4% on the R2R-CE benchmark—comparable to state-of-the-art methods.\nIn summary, our key contributions are threefold:\n•\nWe propose NavForesee, a VLN framework that unifies vision–language model (VLM) planning with world model prediction for navigation tasks.\n•\nWe introduce a hierarchical language planning paradigm that addresses long-instruction, goal-oriented missions by explicitly tracking mission progress and generating concise textual sub-plans.\n•\nWe design a dual-horizon world model prediction mechanism for both short-term execution and long-term milestone navigation, implicitly forming a perception–planning and prediction–action loop that guides agent behavior.\nII\nRelated Works\nII-A\nVisual Language Navigation\nVision-and-Language Navigation (VLN) requires an embodied agent to interpret natural language instructions, perceive visual surroundings, and generate a sequence of actions to reach a specified goal. The advent of large-scale pre-trained VLMs has catalyzed significant progress, largely superseding earlier methods based on topological graphs\n[\n22\n,\n23\n,\n24\n]\n, top-down semantic maps\n[\n25\n,\n26\n,\n27\n]\n, or instruction augmentation\n[\n28\n]\n. Recent works leveraging VLMs can be broadly categorized into two main paradigms.\nThe first uses the VLM as a high-level planner, auto-regressively generating action plans\n[\n29\n,\n30\n,\n31\n]\nor textual trajectories\n[\n32\n]\n. While strong in reasoning, this step-by-step generation is prone to error accumulation and slow inference. The second employs the VLM as an end-to-end policy, directly mapping inputs to actions. However, this often leads to overfitting on training scenes and underutilizes the VLM’s high-level reasoning capabilities.\nTo bridge the gap between these two approaches, dual-system architectures have been proposed\n[\n6\n,\n33\n]\n. These models often adopt a ”Fast-and-Slow” reasoning paradigm, combining a deliberative ”slow” system for high-level reasoning with a lightweight ”fast” reactive controller for low-level execution. Reinforcement learning is frequently employed to align the outputs of both systems and bootstrap the learning of coherent reasoning-action patterns. Despite this progress, a fundamental challenge remains: long, complex reasoning chains (e.g., long CoTs) do not always align with the spatial and dynamic realities of the environment. Furthermore, frequent or periodic elaborate reasoning processes may be unnecessary, as human navigation often relies on simpler, high-level semantic plans rather than continuous, detailed deliberation.\nII-B\nNavigation World Model\nA world model is designed to learn a predictive model of an environment, forecasting future states from historical observations and optional conditioning information, such as actions or instructions. Predictions can be generated in either raw pixel space or a more compact latent space\n[\n18\n]\n. The concept has gained significant traction recently, propelled by large-scale video generation models like Sora(\n[\n34\n]\n, which can produce long-term, consistent, even interactive video sequences from text prompts. A key application of world models in robotics is to serve as a simulation engine, allowing an agent to ”imagine” the outcomes of different action sequences and evaluate control policies before execution\n[\n16\n,\n19\n]\n.\nIn the context of visual navigation, recent works have begun to leverage world models to provide agents with environmental foresight. For instance, NavMorph utilizes a Recurrent State-Space Model (RSSM) to model environmental dynamics in a compact latent space, refining the agent’s policy with imagined future states\n[\n11\n]\n. Similarly, HNR\n[\n9\n]\nadvocates for predicting multi-level semantic features instead of raw pixels, enabling faster and higher-quality imagination to evaluate multiple next-step actions in parallel. Other approaches, like NWM\n[\n35\n]\n, use a controlled video generation model to plan entire trajectories through simulation.\nDespite their promise, existing world models for navigation face two primary limitations. First, action-conditioned models that rely on extensive trajectory sampling and evaluation are often computationally prohibitive, rendering them infeasible for deployment on resource-constrained agents. Second, and more critically for our work, prior research has focused almost exclusively on learning environmental dynamics, largely neglecting to integrate this predictive capability with the high-level language reasoning abilities of modern VLMs. This separation leaves a critical gap, which our work aims to address by unifying these two powerful paradigms.\nFigure 1\n:\nVLM-driven hierarchical navigation plan dataset generation. Episodes from R2R-CE and RxR-CE are processed by\nGemini 2.5 Pro\n, which decomposes long instructions into sub-instructions and identifies keyframe milestones. To generation of waypoint-level reasoning labels, waypoints are sampled between milestones annotated with a navigation summary, future plan, and action (\nforward\n,\nleft\n,\nright\n, or\nstop\n).\nIII\nMethods\nFigure 2\n:\nOverall architecture of NavForesee. The model is built on the Qwen2.5-VL-3B-Instruct backbone, integrating two complementary functionalities: (1) VLM-based hierarchical planning and (2) world model-based dual-horizon visual prediction. For hierarchical planning, textual instruction and visual observations are encoded via Qwen’s original multimodal encoders to produce auto-regressive sub-goal plans. For prediction, a position encoder encodes the agent’s relative pose, and short- and long-horizon dream queries (depth and semantic subqueries) are appended to multimodal embeddings. These queries, processed through structured attention, feed lightweight convolutional decoders for environmental predictions and an MLP head for navigation actions.\nIII-A\nProblem Formulation\nWe target instruction-guided navigation missions in which an embodied agent must interpret a natural language instruction\nl\nl\nand navigate from a given start position to an intended goal location, strictly following the described route. The challenge lies in robustly understanding the instruction, maintaining situational awareness over long horizons, and deciding actions that lead to successful navigation in unseen environments.\nAt time step\nt\nt\n, the agent perceives the environment and obtains a panoramic RGB observation\no\nt\no_{t}\n. It maintains a memory of the past\nH\nH\nobservations,\nO\nt\n−\nH\n:\nt\n−\n1\n=\n[\no\nt\n−\nH\n,\n…\n,\no\nt\n−\n1\n]\nO_{t-H:t-1}=[o_{t-H},\\dots,o_{t-1}]\n, to support temporal reasoning. The navigation policy produces a sequence of\nK\nK\nfuture waypoints\nw\nt\n:\nt\n+\nK\n∈\nℝ\nK\n×\n5\nw_{t:t+K}\\in\\mathbb{R}^{K\\times 5}\n, where each waypoint is defined as\nw\nt\n=\n[\nx\nt\n,\ny\nt\n,\nsin\n⁡\nθ\nt\n,\ncos\n⁡\nθ\nt\n,\nc\nt\n]\n,\nw_{t}=\\left[x_{t},y_{t},\\sin\\theta_{t},\\cos\\theta_{t},c_{t}\\right],\nwith\n(\nx\nt\n,\ny\nt\n)\n(x_{t},y_{t})\ndenoting planar positions,\nθ\nt\n\\theta_{t}\nthe heading angle, and the binary flag\nc\nt\nc_{t}\nindicating whether a\nstop\naction should be triggered. Unless all predicted actions are marked as\nstop\n, the agent continuously moves following the generated waypoints.\nTo solve this problem, we adopt Qwen2.5-VL as our backbone and extend it with two complementary modules. First, we enable\nhierarchical planning\nby decomposing the full instruction into sequential sub-instructions, identifying completed ones and predicting the next step under the current context—leveraging the model’s language understanding capabilities and pretraining on our constructed dataset. Second, we integrate\nworld model foresight\nfor predicting short- and long-term environmental changes, enhancing vision–language coherence and yielding more reliable action policies. Together, these capabilities allow the agent to imitate human navigation behaviors, combining explicit language planning with implicit spatiotemporal prediction.\nIII-B\nVLM-driven Hierarchical Planning Dataset\nWe construct a hierarchical language planning dataset specifically for instruction-guided navigation missions, leveraging advanced Vision–Language Models (VLMs) for multi-modal understanding and sequence analysis. Our goal is to provide training data that captures both short-term execution steps and long-term navigation milestones.\nAs illustrated in Figure\n1\n, we start from public Vision-and-Language Navigation (VLN) benchmarks—R2R-CE (10k episodes) and RxR-CE (20k episodes)—which provide paired natural language instructions and full image observation sequences. Each episode is processed with\nGemini 2.5 Pro\n, guided by a custom prompt template that specifies the model’s role, defines the mission, outlines analytical steps, and enforces an explicit output format. The VLM systematically decomposes each long instruction into a series of sequential sub-instructions, while identifying a dense visual chain of keyframes representing navigation milestones. For paths involving extended travel or sharp turns, we require the inclusion of intermediate milestones to maintain visual continuity in the generated plan. This hierarchical structure enables downstream world models to better learn both short-term and long-term prediction.\nFor every annotated episode, the output is standardized to include: the milestone frame index, the textual description of the completed sub-instruction, and the upcoming planned instruction. Post-processing involves filtering incomplete annotations, correcting logical inconsistencies in the VLM outputs, and converting each episode into multiple navigation segments. We sample waypoints along each trajectory, with each waypoint forming the endpoint of a segment between milestones. Each sampled waypoint is assigned a\nplanning label\ncomprising: (1) a navigation summary (completed sub-instruction), (2) a future plan (next instruction), and (3) a language action (\nforward\n,\nleft\n,\nright\n,\nstop\n).\nThis pipeline produces approximately\n1.3\n1.3\nM training samples from RxR-CE and\n0.2\n0.2\nM from R2R-CE. To ensure balanced training data, we down-sample over-represented straight-motion cases and augment examples involving stopping actions. The final dataset provides richly annotated, balanced samples for training the hierarchical language planning and predictive modules in NavForesee.\nIII-C\nModel Architecture\nOverall Architecture\nThe overall architecture of NavForesee is illustrated in Figure\n2\n. We adopt Qwen2.5-VL-3B-Instruct\n[\n36\n]\n, a large-scale vision–language model with strong multi-modal understanding capabilities, as the backbone. NavForesee is designed to integrate two complementary functionalities: VLM-based language planning and World model-based visual prediction.\nCorrespondingly, we define two primary training objectives: VLM planning training and world model training. Training data from both tasks are jointly mixed to ensure that the model preserves its multi-modal planning ability, while simultaneously extending its capability to generate visual features.\nFor the VLM planning training, textual planning data are directly fed into Qwen for auto-regressive training, leveraging its original text encoder and image encoder components without modification.\nFor the world model training, we introduce an additional position encoder (pos-encoder) to encode the agent’s relative position and orientation from image observations. Two sets of dream queries—corresponding to short- and long-horizon predictions—are appended to the multi-modal embeddings. Each set of dream queries includes depth and semantics subqueries, enabling dual-horizon prediction. Furthermore, an action query, alongside the dream queries, is integrated into the multi-modal inputs and processed by Qwen2.5-VL via a structured attention mechanism. Lightweight convolutional layers serve as decoders to transform dream embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nStructured Attention Mask\nTo maintain a clear separation between short- and long-horizon predictions and to avoid cross-type contamination, each dream query type (depth and semantics) is explicitly decomposed into short-horizon and long-horizon components. As illustrated in Figure\n2\n, we design a structured attention mask tailored for dual-horizon prediction. Long-horizon predictions naturally depend on short-horizon predictions, using them as guidance to ensure temporal coherence. Mutual attention between depth and semantics queries is masked to prevent cross-modal leakage or unintended feature mixing. In contrast, the action query attends to all available information—including past context and both horizons of dream queries—enabling it to make globally consistent navigation predictions.\nIII-D\nDual-horizon World Model Prediction\nSpecifically, to enable accurate dual-horizon environmental feature prediction, we employ the world model architecture that serves as guidance for learning the inverse dynamics of a navigation agent. Here, short-term prediction refers to generating forecasts for\nk\nk\nsteps ahead, while long-term prediction targets navigation milestones, corresponding to an adaptive horizon determined by progress toward the next milestone.\nFor visual feature prediction within Qwen2.5-VL, we introduce two sets of learnable dream queries, namely the short-term\nQ\nS\n∈\nℝ\nL\n×\nd\nQ_{S}\\in\\mathbb{R}^{L\\times d}\nand and long-term\nQ\nL\n∈\nℝ\nL\n×\nd\nQ_{L}\\in\\mathbb{R}^{L\\times d}\nto, which extract temporally aligned feature embeddings specialized for prediction at distinct horizons. To enhance the model’s capability in capturing spatial-temporal correlations and learning environmental dynamics, we further integrate position-orientation state embeddings\ns\nt\n−\nH\n:\nt\ns_{t-H:t}\nfor each input frame through an encoder\nh\n(\n.\n)\nh(.)\n.\nThese dream queries are concatenated with textual instruction embeddings\nl\nl\nand visual observation sequences\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand processed by the Qwen2.5-VL backbone\nf\n(\n.\n)\nf(.)\n. Specially,\nE\nS\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n|\nQ\nS\n)\n,\n\\displaystyle E_{S}=f(l,O_{t-H:t},h(s_{t-H:t})|Q_{S}),\nE\nL\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n|\nQ\nL\n)\n\\displaystyle E_{L}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S}|Q_{L})\nwhere causal attention masking ensures auto-regressive generation: short-term embeddings are produced first, and long-term embeddings are conditioned on short-term predictions.\nWe design lightweight decoders to interpret\nE\nL\nE_{L}\nand\nE\nS\nE_{S}\ninto predicted depth\nd\np\nd_{p}\n, and high-level semantics\nc\np\nc_{p}\n(e.g. derived from DINOV2, SAM). Short-term predictions correspond to a fixed horizon\nk\nk\nwhereas long-term predictions adaptively extrapolate over\nM\nt\nM_{t}\nsteps, dependent on the agent’s progress toward the next milestone:\np\nt\n+\nk\n=\nD\n​\n(\nE\nS\n)\n=\n[\nd\np\n​\n(\nt\n)\n,\nc\np\n​\n(\nt\n)\n]\n,\n\\displaystyle p_{t+k}=D(E_{S})=[d_{p}(t),c_{p}(t)],\np\nt\n+\nM\nt\n=\nD\n​\n(\nE\nL\n)\n=\n[\nd\np\n​\n(\nt\n+\nM\nt\n)\n,\nc\np\n​\n(\nt\n+\nM\nt\n)\n]\n\\displaystyle p_{t+M_{t}}=D(E_{L})=[d_{p}(t+M_{t}),c_{p}(t+M_{t})]\nIII-E\nPredictive Action Policy Learning\nGiven two temporally order states\no\nt\no_{t}\nand\no\nt\n+\n1\no_{t+1}\n, the intermediate action\na\n^\n​\n(\nt\n)\n\\hat{a}(t)\ncan be inferred via inverse dynamics. We leverage this principle to learn an action policy conditioned on the instruction\nl\nl\n, historical visual observations\nO\nt\n−\nH\n:\nt\nO_{t-H:t}\nand the dual-horizon predictive latent features\nE\nS\nE_{S}\nand\nE\nL\nE_{L}\ngenerated by the world model.\nTo enhance the encoding of task-relevant context for action prediction, we introduce a learnable action query\nQ\na\nQ_{a}\n. This query is concatenated with the dream queries and multi-modal input embeddings to form a unified action embedding. The Qwen2.5-VL backbone processes these embeddings to produce the contextual representation for action inference, which is subsequently projected into the action space:\nE\na\n=\nf\n​\n(\nl\n,\nO\nt\n−\nH\n:\nt\n,\nh\n​\n(\ns\nt\n−\nH\n:\nt\n)\n,\nQ\nS\n,\nQ\nL\n|\nQ\na\n)\n\\displaystyle E_{a}=f(l,O_{t-H:t},h(s_{t-H:t}),Q_{S},Q_{L}|Q_{a})\na\n^\nt\n:\nt\n+\nk\n=\nM\ni\n​\nn\n​\nv\n​\n(\nE\nS\n,\nE\nL\n|\nE\na\n)\n\\displaystyle\\hat{a}_{t:t+k}=M_{inv}(E_{S},E_{L}|E_{a})\nwhere\nE\na\nE_{a}\nis the action embedding and\nM\ni\n​\nn\n​\nv\nM_{i}nv\ndenotes the inverse dynamics model.\nNotably, in our action policy learning pipeline, the action embedding\nE\na\nE_{a}\nis extracted through the Qwen2.5-VL backbone, while action predictions are primarily conditioned on the dual-horizon predictive features, ensuring that decisions are informed by both past observations and forecasted environmental dynamics\nIII-F\nClose the Planning/Prediction and Action Loop\nFor VLM planning training, we finetune Qwen2.5-VL model based on the constructed dataset in an auto-regressive manner independently to build a powerful model capable of conducting hierarchical navigation.\nFor world model prediction and action policy learning, the training tasks are classified depth prediction, semantic feature prediction and action prediction. Depth prediction error\nL\nd\nL_{d}\nis measured using the Scale-invariant Logarithmic Loss (SiLogLoss) at the pix-level level. The semantics feature prediction error\nL\nc\nL_{c}\nand action error\nL\na\nL_{a}\nare computed using mean squared error (MSE). The overall training loss\nL\nL\ncomprise\nL\nd\nL_{d}\n,\nL\nc\nL_{c}\nand\nL\na\nL_{a}\nL\n=\nα\n​\nL\nd\n+\nβ\n​\nL\nc\n+\nL\na\nL=\\alpha L_{d}+\\beta L_{c}+L_{a}\nwhere\nα\n\\alpha\nand\nβ\n\\beta\nare weighting hyperparameters balancing the tasks.\nIV\nExperimental Evaluation\nTABLE I:\nComparison with other methods on the Val-Unseen split of R2R-CE and RxR-CE\nMethod\nObservation\nR2R-CE Val-Unseen\nRxR-CE Val-Unseen\nS.RGB\nPano.\nDepth\nOdo\nNE\n↓\n\\downarrow\nOS\n↑\n\\uparrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSR\n↑\n\\uparrow\nSPL\n↑\n\\uparrow\nHPN+DN*\n[\n37\n]\n✓\n✓\n✓\n6.31\n40.0\n36.0\n34.0\n-\n-\n-\nCMA*\n[\n38\n]\n✓\n✓\n✓\n6.20\n52.0\n41.0\n36.0\n8.76\n26.5\n22.1\nSim2Sim\n[\n39\n]\n✓\n✓\n✓\n6.07\n52.0\n43.0\n36.0\n8.76\n26.5\n22.1\nGridMM*\n[\n8\n]\n✓\n✓\n✓\n5.11\n61.0\n49.0\n41.0\n-\n-\n-\nDreamWalker*\n[\n40\n]\n✓\n✓\n✓\n5.53\n59.0\n49.0\n44.0\n-\n-\n-\nReborn*\n[\n41\n]\n✓\n✓\n✓\n5.40\n57.0\n50.0\n46.0\n5.98\n48.6\n42.0\nETPNav*\n[\n42\n]\n✓\n✓\n✓\n4.71\n65.0\n57.0\n49.0\n5.64\n54.7\n44.8\nHNR*\n[\n9\n]\n✓\n✓\n✓\n4.42\n67.0\n61.0\n51.0\n5.50\n56.3\n46.7\nAG-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n39.0\n23.0\n19.0\n-\n-\n-\nR2R-CMTP\n[\n43\n]\n✓\n✓\n✓\n7.90\n38.0\n26.0\n22.0\n-\n-\n-\nInstruc-Nav\n[\n31\n]\n✓\n✓\n✓\n6.89\n-\n31.0\n24.0\n-\n-\n-\nLAW\n[\n44\n]\n✓\n✓\n✓\n6.83\n44.0\n35.0\n31.0\n10.90\n8.0\n8.0\nCM2\n[\n45\n]\n✓\n✓\n✓\n7.02\n41.0\n34.0\n27.0\n-\n-\n-\nWS-MGMap\n[\n46\n]\n✓\n✓\n✓\n6.28\n47.0\n38.0\n34.0\n-\n-\n-\nAO-Planner\n[\n47\n]\n✓\n✓\n5.55\n59.0\n47.0\n33.0\n-\n-\n-\nSeq2Seq\n[\n48\n]\n✓\n✓\n7.77\n37.0\n25.0\n22.0\n12.10\n13.9\n11.9\nCMA\n[\n48\n]\n✓\n✓\n7.37\n40.0\n32.0\n30.0\n-\n-\n-\nNA Vid\n[\n49\n]\n✓\n5.47\n49.0\n37.0\n35.0\n-\n-\n-\nUni-NA Vid\n[\n50\n]\n✓\n5.58\n53.5\n47.0\n42.7\n6.24\n48.7\n40.9\nNaVILA\n[\n51\n]\n✓\n5.22\n62.5\n54.0\n49.0\n6.77\n49.3\n44.0\nStream VLN\n[\n52\n]\n✓\n4.98\n64.2\n56.9\n51.9\n6.22\n52.9\n46.0\nCorrectNav\n[\n53\n]\n✓\n4.24\n67.5\n65.1\n62.3\n4.09\n69.3\n63.3\nNavForesee(Ours)\n✓\n3.94\n78.4\n66.2\n59.7\n4.20\n66.3\n53.2\nTABLE II:\nPerformance comparison between VLM planning and dual-horizon world model prediction\nIndex\nVLM planning\nLong-term prediction\nShort-term prediction\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n✓\n48.8%\n75.5%\n5.61\n39.4%\n3\n✓\n✗\n✓\n58.6%\n76.4%\n4.47\n50.1%\n4\n✗\n✗\n✗\n52.6%\n67.4%\n5.53\n46.7%\nWe evaluate our model in continuous environment of the Habitant simulator on the R2R-CE and RxR-CE datasets.\nR2R-CE\ndataset is derived from the Matterport3D indoor environments, discretized for path planning but operationalized in the Habitat simulator under a continuous navigation setting. It provides fine-grained, step-by-step natural language instructions, allowing for detailed guidance at each navigation step. In the simulator, the embodied agent\ncan execute turns as small as\n15\n∘\n15^{\\circ}\nand perceives the scene through a\n90\n∘\n90^{\\circ}\nhorizontal field-of-view.\nRxR-CE\nis a large-scale, multilingual VLN dataset comprising about 126K human-annotated instructions. Compared to R2R-CE, RxR-CE covers more diverse and complex trajectories, increasing the difficulty of the navigation tasks. The agent in this setting uses a coarser minimum turn\nincrement of\n30\n∘\n30^{\\circ}\nand a narrower\n79\n∘\n79^{\\circ}\nhorizontal field-ofview, which demands more deliberate movement planning for effective scene coverage.\nWe evaluate navigation performance using standard metrics including success rate (SR), oracle success rate (OS), success weighted by path length (SPL), and navigation error (NE).\nIV-A\nComparison with State-of-the-Art Methods\nTable 1 reports the performance of the proposed method compared with other approaches on the R2R‑CE and RxR‑CE datasets. Overall, NavForesee delivers competitive results against state‑of‑the‑art (SOTA) methods.\nSpecifically, on the val unseen split of the R2R‑CE dataset, NavForesee achieves SOTA performance by improving SR by 1.1%, OSR by 10.9%, and reducing NE by 0.3 m. This improvement can be attributed to the integration of the world model prediction module, which enables the agent to better capture environmental dynamics, avoid obstacles, and explore the surroundings more effectively.\nIn contrast, NavForesee performs slightly worse than SOTA methods on RxR‑CE, indicating limited generalization to more complex environments. It is worth noting that we train soly on NavForesee on R2R‑CE and RxR‑CE datasets, whereas other methods exploit diverse and large‑scale datasets to enhance generalization.\nAlthough NavForesee does not consistently outperform all baselines, it achieves the highest OSR across both datasets. This demonstrates the value of incorporating world model prediction into VLN agents and implies its promising potential for future vision‑and‑language navigation tasks.\nIV-B\nAblation Study\nAs shown in Table\nII\n, removing any of the three key modules—hierarchical VLM planning, long-term prediction, or short-term prediction—results in clear performance degradation. The full NavForesee model, which combines all modules, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their integration. Without VLM planning, the success rate drops sharply to 48.8% and the SPL decreases by more than 16 points, reflecting the importance of explicit instruction decomposition and progress tracking for efficient navigation. Disabling long-term prediction also leads to a noticeable reduction in SR (58.6%) and higher NE, highlighting the role of milestone foresight in providing strategic guidance over extended trajectories. When all three modules are removed, navigation quality deteriorates the most, confirming that planning and both prediction horizons together are crucial for accurate, efficient long-horizon navigation.\nFigure 3\n:\nShort-term depth and semantics predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, future depth prediction for future frames, semantics predictions for future frames. Semantic features are DinoV2 features and visualized by a pretrained segmentation head. Instructions: UP the stairs. Turn to the left and enter into the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom\nFigure 4\n:\nNavForesee’s geometric-semantic feature imagination across different motion modes. The model accurately predicts environmental dynamics in straight motion, generalizes effectively to turning scenarios, and infers detailed object geometry and depth distribution from minimal visual input, such as a brief glimpse into a room\nIV-C\nQualitative Analysis\nFigure\n3\nillustrates the short-term depth and semantic feature predictions generated by our world model over the course of a complete navigation episode, forecasting up to four future steps. Although the predicted depth maps appear somewhat coarse—owing to the constraints of pixel-level supervised training on R2R-CE and RxR-CE—they nonetheless preserve the scene’s global geometry and spatial layout, faithfully capturing agent movements such as ascending or descending staircases, entering and exiting rooms, and making sharp or gradual turns. This ability to retain high-level spatial coherence despite reduced pixel detail ensures that the model’s predictions remain informative for downstream navigation decisions. The semantics predictions, obtained via a pretrained segmentation head, exhibit strong alignment with ground truth labels, successfully reflecting dynamic environmental changes in synchrony with the agent’s actions.\nFigure\n4\nfurther provides detailed examples that showcase NavForesee’s ability to imaginatively anticipate semantic features across diverse motion patterns. In addition to delivering accurate environment dynamics predictions when following a straightforward trajectory, NavForesee demonstrates remarkable generalization by reliably extrapolating future geometric and semantic structures when performing more complex navigational behaviors such as turns. In the final scenario, the agent receives only a brief partial observation—a quick glance into a room—yet the model is able to produce a vivid and coherent internal imagination of the room’s layout. This includes accurately inferring the relative shape and position of the bed, as well as estimating the depth distribution across the room, thus indicating its capacity to reason about unseen spatial regions.\nV\nConclusion\nWe proposed NavForesee, a vision–language navigation framework that unifies hierarchical language planning with dual-horizon predictive world modeling. By decomposing long instructions into sub-goals and anticipating both short-term dynamics and long-term milestones, NavForesee forms an implicit perception–planning and prediction–action loop.\nExperiments on R2R-CE and RxR-CE show strong performance—66.2% SR and 78.4% OSR on R2R-CE—comparable to state-of-the-art despite training only on public data. Qualitative results further reveal solid depth and semantics predictions that guide agent decisions in complex scenarios.\nThese findings highlight the benefit of equipping embodied agents with foresight: by “foreseeing” future states, NavForesee effectively fuses language planning with spatiotemporal imagination to improve visual-language navigation.\nReferences\n[1]\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,\nY. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, “Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality,” March\n2023. [Online]. Available:\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[2]\nS. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen,\n“Ll3da: Visual interactive instruction tuning for omni-3d understanding,\nreasoning, and planning,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2311.18651\n[3]\nA.-M. Halacheva, J.-N. Zaech, X. Wang, D. P. Paudel, and L. V. Gool,\n“Gaussianvlm: Scene-centric 3d vision-language models using language-aligned\ngaussian splats for embodied reasoning and beyond,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2507.00886\n[4]\nH. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng,\nY. Zhao, J. Pang, and Z. Zhao, “Chat-scene: Bridging 3d scene and large\nlanguage models with object identifiers,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2312.08168\n[5]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.\n[Online]. Available:\nhttps://arxiv.org/abs/2304.08485\n[6]\nQ. Liu, T. Huang, Z. Zhang, and H. Tang, “Nav-r1: Reasoning and navigation in\nembodied scenes,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.10884\n[7]\nH. Zhou, J. Yu, and W. Yang, “Dual memory units with uncertainty regulation\nfor weakly supervised video anomaly detection,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.05160\n[8]\nZ. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, “Gridmm: Grid memory map for\nvision-and-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2307.12907\n[9]\nZ. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, “Lookahead\nexploration with neural radiance representation for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2404.01943\n[10]\nX. Zhao, W. Cai, L. Tang, and T. Wang, “Imaginenav: Prompting vision-language\nmodels as embodied navigator through scene imagination,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2410.09874\n[11]\nX. Yao, J. Gao, and C. Xu, “Navmorph: A self-evolving world model for\nvision-and-language navigation in continuous environments,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2506.23468\n[12]\nB. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang,\n“Navcot: Boosting llm-based vision-and-language navigation via learning\ndisentangled reasoning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2403.07376\n[13]\nS. Wang, Y. Wang, W. Li, X. Cai, Y. Wang, M. Chen, K. Wang, Z. Su, D. Li, and\nZ. Fan, “Aux-think: Exploring reasoning strategies for data-efficient\nvision-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2505.11886\n[14]\nQ. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han,\nC. Finn, A. Handa, M.-Y. Liu, D. Xiang, G. Wetzstein, and T.-Y. Lin,\n“Cot-vla: Visual chain-of-thought reasoning for vision-language-action\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2503.22020\n[15]\nY. Huang, J. Zhang, S. Zou, X. Liu, R. Hu, and K. Xu, “Ladi-wm: A latent\ndiffusion-based world model for predictive manipulation,” 2025. [Online].\nAvailable:\nhttps://arxiv.org/abs/2505.11528\n[16]\nJ. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo,\nF. Wang, D. Zhao, and H. Chen, “Worldvla: Towards autoregressive action\nworld model,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2506.21539\n[17]\nE. Karypidis, I. Kakogeorgiou, S. Gidaris, and N. Komodakis, “Dino-foresight:\nLooking into the future with dino,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.11673\n[18]\nF. Baldassarre, M. Szafraniec, B. Terver, V. Khalidov, F. Massa, Y. LeCun,\nP. Labatut, M. Seitzer, and P. Bojanowski, “Back to the features: Dino as a\nfoundation for video world models,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.19468\n[19]\nW. Zhang, H. Liu, Z. Qi, Y. Wang, X. Yu, J. Zhang, R. Dong, J. He, F. Lu,\nH. Wang, Z. Zhang, L. Yi, W. Zeng, and X. Jin, “Dreamvla: A\nvision-language-action model dreamed with comprehensive world knowledge,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.04447\n[20]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[21]\nA. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:\nMultilingual vision-and-language navigation with dense spatiotemporal\ngrounding,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.07954\n[22]\nY. Hong, C. Rodriguez-Opazo, Y. Qi, Q. Wu, and S. Gould, “Language and visual\nentity relationship graph for agent navigation,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.09304\n[23]\nZ. Deng, K. Narasimhan, and O. Russakovsky, “Evolving graphical planner:\nContextual global planning for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2007.05655\n[24]\nS. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, “Think global, act\nlocal: Dual-scale graph transformer for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2202.11742\n[25]\nM. Z. Irshad, N. C. Mithun, Z. Seymour, H.-P. Chiu, S. Samarasekera, and\nR. Kumar, “Sasra: Semantically-aware spatio-temporal reasoning agent for\nvision-and-language navigation in continuous environments,” 2021. [Online].\nAvailable:\nhttps://arxiv.org/abs/2108.11945\n[26]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[27]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[28]\nS. Zhang, Y. Qiao, Q. Wang, L. Guo, Z. Wei, and J. Liu, “Flexvln: Flexible\nadaptation for diverse vision-and-language navigation tasks,” 2025.\n[Online]. Available:\nhttps://arxiv.org/abs/2503.13966\n[29]\nY. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: Visual language\nnavigation via multi-expert discussions,” in\n2024 IEEE International\nConference on Robotics and Automation (ICRA)\n, 2024, pp. 17 380–17 387.\n[30]\nP. Chen, X. Sun, H. Zhi, R. Zeng, T. H. Li, G. Liu, M. Tan, and C. Gan,\n“\na\n2\na^{2}\nnav: Action-aware zero-shot robot navigation by exploiting\nvision-and-language ability of foundation models,” 2023. [Online].\nAvailable:\nhttps://arxiv.org/abs/2308.07997\n[31]\nY. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, “Instructnav: Zero-shot system\nfor generic instruction navigation in unexplored environment,” 2024.\n[Online]. Available:\nhttps://arxiv.org/abs/2406.04882\n[32]\nY. Wang, Y. Fang, T. Wang, Y. Feng, Y. Tan, S. Zhang, P. Liu, Y. Ji, and R. Xu,\n“Dreamnav: A trajectory-based imaginative framework for zero-shot\nvision-and-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.11197\n[33]\nX. Xue, J. Hu, M. Luo, X. Shichao, J. Chen, Z. Xie, Q. Kuichen, G. Wei, M. Xu,\nand Z. Chu, “Omninav: A unified framework for prospective exploration and\nvisual-language navigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2509.25687\n[34]\nY. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun,\nJ. Gao, L. He, and L. Sun, “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models,” 2024. [Online].\nAvailable:\nhttps://arxiv.org/abs/2402.17177\n[35]\nA. Bar, G. Zhou, D. Tran, T. Darrell, and Y. LeCun, “Navigation world\nmodels,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.03572\n[36]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang,\nJ. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu,\nY. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and\nJ. Lin, “Qwen2.5-vl technical report,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.13923\n[37]\nJ. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets, “Waypoint models\nfor instruction-guided navigation in continuous environments,” 2021.\n[Online]. Available:\nhttps://arxiv.org/abs/2110.02207\n[38]\nY. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap between learning in\ndiscrete and continuous environments for vision-and-language navigation,”\n2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02764\n[39]\nJ. Krantz and S. Lee, “Sim-2-sim transfer for vision-and-language navigation\nin continuous environments,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.09667\n[40]\nH. Wang, W. Liang, L. V. Gool, and W. Wang, “Dreamwalker: Mental planning for\ncontinuous vision-language navigation,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2308.07498\n[41]\nD. An, Z. Wang, Y. Li, Y. Wang, Y. Hong, Y. Huang, L. Wang, and J. Shao, “1st\nplace solutions for rxr-habitat vision-and-language navigation competition\n(cvpr 2022),” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2206.11610\n[42]\nD. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, “Etpnav:\nEvolving topological planning for vision-language navigation in continuous\nenvironments,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2304.03047\n[43]\nK. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, “Topological\nplanning with transformers for vision-and-language navigation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2012.05292\n[44]\nS. Raychaudhuri, S. Wani, S. Patel, U. Jain, and A. X. Chang,\n“Language-aligned waypoint (law) supervision for vision-and-language\nnavigation in continuous environments,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2109.15207\n[45]\nG. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and\nK. Daniilidis, “Cross-modal map learning for vision and language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.05137\n[46]\nP. Chen, D. Ji, K. Lin, R. Zeng, T. H. Li, M. Tan, and C. Gan,\n“Weakly-supervised multi-granularity map learning for vision-and-language\nnavigation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.07506\n[47]\nJ. Chen, B. Lin, X. Liu, L. Ma, X. Liang, and K.-Y. K. Wong,\n“Affordances-oriented planning using foundation models for continuous\nvision-language navigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2407.05890\n[48]\nJ. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, “Beyond the\nnav-graph: Vision-and-language navigation in continuous environments,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/2004.02857\n[49]\nJ. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and\nH. Wang, “Navid: Video-based vlm plans the next step for vision-and-language\nnavigation,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.15852\n[50]\nJ. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and\nH. Wang, “Uni-navid: A video-based vision-language-action model for unifying\nembodied navigation tasks,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.06224\n[51]\nA.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin,\nS. Liu, and X. Wang, “Navila: Legged robot vision-language-action model for\nnavigation,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2412.04453\n[52]\nM. Wei, C. Wan, X. Yu, T. Wang, Y. Yang, X. Mao, C. Zhu, W. Cai, H. Wang,\nY. Chen, X. Liu, and J. Pang, “Streamvln: Streaming vision-and-language\nnavigation via slowfast context modeling,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2507.05240\n[53]\nZ. Yu, Y. Long, Z. Yang, C. Zeng, H. Fan, J. Zhang, and H. Dong, “Correctnav:\nSelf-correction flywheel empowers vision-language-action navigation model,”\n2025. [Online]. Available:\nhttps://arxiv.org/abs/2508.10416\nSupplementary Material\nI\nImplementation Details\nI-A\nModel Architecture\nBase Model\nWe employ Qwen2.5-VL-3B-Instruct\n[\n36\n]\nas the backbone of NavForesee. It adopts the Qwen2.5 LLM as its text decoder and integrates a vision encoder. The vision encoder utilizes a Vision Transformer (ViT) architecture to encode visual observations, while the text decoder is responsible for generating the hierarchical planning outputs and action trunk predictions. Detailed descriptions of Qwen2.5-VL can be found in\n[\n36\n]\n. For hierarchical planning, we directly use the original multimodal encoders and text decoder of Qwen2.5-VL without any modifications. For world model prediction and action policy learning, we introduce a position encoder to represent the agent’s relative position and orientation derived from image observations.\nLightweight decoders transform the dream query embeddings into environmental predictions (depth and semantics), while a simple MLP predicts action outputs (waypoints, orientation estimates, and arrival flags).\nDream Query Design\nTwo sets of dream queries (short-term and long-term), along with an action query, are appended to the multimodal embeddings. Each set of dream queries contains depth and semantics subqueries, enabling dual-horizon prediction. We use DINOv2 and SAM features as semantic representations. Thus, there are six query subsets in total—depth, DINOv2, and SAM for both short-term and long-term horizons—with each subset consisting of 64 tokens. The action query consists of a single token dedicated to action prediction.\nWorld Model Decoders\nWe design task-specific lightweight world model decoders to transform dream embeddings into depth maps, semantic features, and actions. For depth and semantics predictions, we employ decoder architectures with identical design: dream embeddings and a set of learnable masks are processed by a 2-layer ViT-based decoder to produce predicted features. Additionally, we apply the decoder from VQ-VAE to render depth features into depth maps.\nAction Prediction\nThe action prediction module takes the action embedding produced by Qwen2.5-VL as input and generates predicted waypoints, orientation estimates, and arrival flags. First, a 2-layer transformer processes the action embedding to capture dependencies on the world model’s dream embeddings. Then, the processed action embedding is passed to the action prediction head, which outputs the final navigation predictions, including waypoints, orientation estimates, and arrival flags. The action prediction head consists of a simple MLP with two linear layers and a ReLU activation in between.\nI-B\nTraining Details\nWe interleave the VLM planning training data and world model training data to jointly train NavForesee. The training batch size is set to 4, and the number of image observations is flexible, up to a maximum length of 20. Depth and semantic features are precomputed and loaded during training. We use the AdamW optimizer with an initial learning rate of\n1\n×\n10\n−\n5\n1\\times 10^{-5}\n. Depth and semantics predictions are weighted with\nα\n=\n0.25\n\\alpha=0.25\nand\nβ\n=\n0.3\n\\beta=0.3\n. The model is trained for a total of 3 epochs on 64 NVIDIA H20 GPUs, with ViT parameters frozen. The fixed short-term prediction horizon is set to 4, and the number of predicted waypoints is set to 5.\nII\nExperimental Evaluations\nII-A\nHierarchical Planning Evaluation\nTo evaluate the hierarchical planning capabilities of NavForesee, we conduct experiments on the Val-Unseen split of the R2R-CE and RxR-CE datasets. An example is illustrated in Figure\n5\n. We perform hierarchical planning for each step of an episode. NavForesee generates a navigation summary, plan, and actions strictly following the output format specified in the prompt template. Apart from the initial position, NavForesee consistently identifies milestones along the route, summarizes completed sub-instructions, and formulates the next sub-instruction in alignment with the overall instruction context. This demonstrates that NavForesee effectively leverages its multimodal understanding capabilities to decompose complex navigation tasks into manageable sub-goals, thereby enabling more structured and efficient navigation. Notably, the hierarchical planning module is jointly trained with the world model prediction and action policy learning modules, indicating that NavForesee maintains strong language planning capabilities even when extended with additional functionalities. Furthermore, the hierarchical plans are precise and concise, which greatly benefits subsequent navigation decisions.\nFigure 5\n:\nHierarchical planning examples generated by NavForesee for the instruction ”Go up the stairs and straight forward the doorway. Turn right, move forward, and enter the doorway on the right. Move forward into the bedroom and stop in front of the toilet”. From top to bottom: frames with timestamps, global navigation map, and navigation planning outputs. NavForesee accurately identifies milestones along the route, summarizes completed sub-instructions, and generates the next sub-instruction in accordance with the instruction context.\nFigure 6\n:\nShort-term and long-term depth predictions. From top to bottom: frames with timestamps, future ground truth frames with timestamps, short-term depth predictions for future frames, and long-term depth predictions for milestones. Instruction: ”Up the stairs. Turn to the left and enter the second open door on the left. Walk towards the foot of the bed. Turn right and enter the open door to the bathroom.”\nII-B\nShort-term and Long-term Prediction Evaluation\nFigure\n6\nillustrates the short-term and long-term depth predictions produced by our world model over a complete navigation episode. Short-term predictions forecast up to four future steps, whereas long-term predictions extrapolate over an adaptive horizon determined by progress towards the next milestone. Compared to short-term predictions, long-term depth predictions may be less accurate in capturing detailed depth at milestone locations, since milestone positions are unknown during inference. At the beginning of the episode, the long-term predictions effectively capture the scene when the agent ascends the stairs. As the agent approaches the first milestone (the doorway), the long-term predictions degrade slightly, likely due to the increased uncertainty of longer horizons and the absence of explicit milestone information. In such cases, long-term predictions tend to track short-term outputs, because long-term queries can attend to short-term queries. Nevertheless, the long-term predictions maintain the overall scene layout and depth distribution, providing valuable guidance for strategic navigation. This demonstrates that NavForesee’s world model effectively anticipates environmental changes over both short and long horizons, enhancing the agent’s planning and action capabilities in complex scenarios.\nTABLE III\n:\nPerformance comparison between depth prediction and semantics prediction\nIndex\nDepth\nSemantics\nSR\n↑\n\\uparrow\nOSR\n↑\n\\uparrow\nNE\n↓\n\\downarrow\nSPL\n↑\n\\uparrow\n1\n✓\n✓\n66.2%\n78.4%\n3.94\n59.7%\n2\n✗\n✓\n61.8%\n76.7%\n4.37\n54.9%\n3\n✓\n✗\n60.0%\n76.2%\n4.59\n52.9%\nII-C\nAblation Study on Depth and Semantics Predictions\nWe conduct ablation studies to evaluate the individual contributions of depth and semantics predictions in the world model. As shown in Table\nIII\n, removing either depth or semantics predictions results in a clear performance drop. The full NavForesee model, which integrates both depth and semantics predictions, achieves the highest SR (66.2%), OSR (78.4%), lowest NE (3.94), and best SPL (59.7%), validating the benefit of their combination. Without depth prediction, the SR drops to 61.8% and SPL decreases by 4.8 points, highlighting the importance of depth information for spatial reasoning and obstacle avoidance. Disabling semantics predictions leads to an even larger SR reduction (60.0%) and higher NE, underscoring the critical role of semantic features in recognizing landmarks and guiding navigation. These findings confirm that both depth and semantics predictions are essential for accurate and efficient navigation.",
  "preview_text": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.\n\nNavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction\nFei Liu\n∗\nShichao Xie\n∗\nMinghua Luo Zedong Chu\n†\nJunjun Hu Xiaolong Wu\n†\nMu Xu\nAmap, Alibaba Group\n*Joint first authors\n†\n\\dagger\nCorresponding authors\nAbstract\nEmbodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robus",
  "is_relevant": true,
  "relevance_score": 7.0,
  "extracted_keywords": [
    "multimodal generation",
    "unified generation understanding"
  ],
  "one_line_summary": "NavForesee是一个统一的视觉-语言世界模型，通过结合高层次语言规划和预测性世界模型想象，实现分层规划和双视野导航预测，以提升具身导航在复杂场景中的性能。",
  "detailed_summary": "## 论文摘要：NavForesee\n\n**1. 研究背景和动机**\n在具身导航领域，智能体根据复杂的自然语言指令完成长视野导航任务仍面临巨大挑战。现有方法通常存在两大局限：一是**规划与记忆能力不足**，导致在未知环境中容易“迷失”；二是**缺乏预测前瞻性**，模型多为被动反应式，无法主动预测未来环境状态以指导行动。现有研究往往将视觉语言模型（VLM）的规划能力与世界模型的预测能力分开探索，导致规划可能脱离视觉现实，而预测可能偏离指令目标。本文旨在**统一这两种能力**，构建一个能同时进行高层次语义规划和多尺度环境预测的智能体。\n\n**2. 核心方法和技术创新**\n本文提出了 **NavForesee**，一个统一的视觉语言世界模型框架。其核心创新在于：\n- **统一架构**：基于 **Qwen2.5-VL-3B-Instruct** 模型，在一个框架内集成了**分层语言规划**和**双视野世界模型预测**。\n- **分层语言规划**：模型将长指令分解为子指令序列，实时跟踪任务进度（已完成什么、当前目标、下一步计划），生成结构化的高级语义计划。\n- **双视野预测**：作为世界模型，NavForesee 在**潜在特征空间**（而非像素空间）进行预测：\n    - **短期预测**：预测未来几步的环境动态（深度、DINOv2和SAM语义特征），增强局部感知和避障。\n    - **长期预测**：自适应地预测到达下一个导航“里程碑”时的环境特征，提供战略性引导。\n- **结构化训练与闭环**：通过精心构建的数据集（使用大模型自动标注）联合训练规划与预测模块，并设计**结构化注意力掩码**防止不同预测视野间的信息污染，形成了“感知-规划/预测-行动”的内部反馈闭环。\n\n**3. 主要实验结果**\n在 **R2R-CE** 和 **RxR-CE** 两个标准连续环境导航基准上进行了评估：\n- 在 **R2R-CE Val-Unseen** 上取得了极具竞争力的性能：**成功率（SR）达66.2%**，**Oracle成功率（OSR）达78.4%**，导航误差（NE）为3.94米，部分指标达到最先进水平。\n- 在更复杂的 **RxR-CE** 上表现稍逊，但仍展示了良好的泛化能力（SR 66.3%）。\n- **消融实验**证实了所有核心组件的必要性：移除VLM规划导致SR大幅下降至48.8%；移除长期预测SR降至58.6%；同时移除深度和语义预测均会导致性能显著退化。\n- **定性分析**显示，模型能生成连贯的深度和语义预测，有效捕捉上下楼梯、转弯等环境动态，并能根据局部观察“想象”房间内部布局。\n\n**4. 研究意义和价值**\nNavForesee 的工作具有重要的理论和应用价值：\n- **方法论创新**：首次在单一VLM框架内**紧密耦合了显式语言规划与隐式时空预测**，为解决长视野、复杂指令导航问题提供了一个新颖且强大的范式。\n- **性能提升**：实验证明，这种统一方法能显著提升导航的成功率和鲁棒性，尤其在需要长期规划和环境理解的场景中。\n- **启发性强**：其“分层规划”与“双视野预测”的设计灵感来源于人类导航策略，为构建更类人、更智能的具身智能体指明了方向。该研究证明了为智能体赋予“预见”能力，是实现更可靠视觉语言导航的关键。",
  "qa_pairs": [
    {
      "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
      "answer": "## 论文核心分析：NavForesee\n\n### **一、 拟解决的核心问题**\n论文旨在解决**具身导航（Embodied Navigation）** 中，智能体在执行**长视野、复杂自然语言指令**任务时面临的两大根本性挑战：\n\n1.  **规划与记忆缺陷**：现有基于视觉-语言模型（VLM）的导航智能体，其上下文窗口和规划能力有限，在复杂、未见过的环境中容易“迷失”，难以进行鲁棒的长期规划。\n2.  **缺乏预测性远见**：现有模型本质上是“反应式”的，无法主动**预测未来的环境状态**来指导行动，导致在动态或复杂场景中决策失误。\n\n更本质地说，当前研究存在**范式割裂**：专注于语言规划的VLM智能体可能产生与视觉现实脱节的“语义幻觉”；而专注于环境动态预测的世界模型，其预测可能偏离语言指令的目标，产生“语义漂移”。\n\n### **二、 核心创新点**\nNavForesee的核心创新在于**首次在一个统一的VLM框架内，紧密耦合了高层次语言规划与多尺度世界模型预测**，形成了一个内在的感知-规划/预测-行动闭环。具体体现为以下三个关键创新：\n\n1.  **统一的VLM架构**：提出一个单一的模型（基于Qwen2.5-VL-3B），同时具备**分层语言规划**和**双视野世界模型预测**两种能力。这打破了传统上将规划器与世界模型分离设计的范式，实现了两种能力的相互增强。\n2.  **分层语言规划范式**：\n    - **任务解构与进度跟踪**：模型将完整的导航指令分解为一系列顺序的**子指令**，明确追踪哪些子指令已完成，当前处于哪个子指令，并规划下一步的语义动作“主干”。\n    - **作用**：将智能体的高级规划牢牢锚定在整体指令的语义框架内，防止语义漂移。\n3.  **双视野预测世界模型机制**：\n    - **短视野预测**：预测未来几步（如4步）的环境动态特征（深度、语义），用于增强局部环境感知，实现鲁棒的避障和对环境动态的理解。\n    - **长视野预测**：自适应地预测完成**当前子指令**时所抵达的**里程碑（Milestone）** 处的关键视觉特征。这相当于为智能体“想象”出了下一个战略目标点的场景。\n    - **高效特征预测**：借鉴潜在空间世界模型思想，不生成计算昂贵的像素级图像，而是预测一组紧凑的高层特征（深度、DINOv2特征、SAM特征），这些特征包含了关键的几何和语义信息。\n\n### **三、 解决方案与技术路径**\nNavForesee通过一套完整的技术流程实现了上述创新：\n\n1.  **数据构建**：利用强大的VLM（Gemini 2.5 Pro）对公开数据集（R2R-CE, RxR-CE）进行自动标注，生成包含**里程碑、子指令分解、未来计划、动作标签**的层次化规划数据集，用于训练模型的规划能力。\n\n2.  **模型设计**：\n    - **主干网络**：采用Qwen2.5-VL-3B作为多模态理解与生成的基础。\n    - **规划模块**：直接利用主干网络的文本解码器，以自回归方式生成层次化规划文本（总结、计划、动作）。\n    - **预测模块**：\n        - 引入**位置编码器**编码智能体位姿。\n        - 设计**可学习的梦境查询**：分为短视野和长视野两组，每组包含深度和语义子查询。\n        - 设计**结构化注意力掩码**：确保短、长视野预测间的依赖关系（长视野依赖短视野），并防止深度与语义特征间的不当混合。动作查询可以关注所有信息。\n        - **轻量级解码器**：将梦境查询的嵌入解码为预测的深度图和语义特征。\n    - **动作策略模块**：引入一个**动作查询**，其嵌入与梦境查询一同处理，最终通过一个MLP头输出连续的路径点、朝向和停止标志。\n\n3.  **训练与闭环**：\n    - **联合训练**：将规划任务数据与世界模型预测（深度、语义）及动作预测任务的损失混合训练。\n    - **损失函数**：`L = α * L_depth(SiLogLoss) + β * L_semantics(MSE) + L_action(MSE)`。\n    - **形成闭环**：规划产生的子目标指导世界模型进行**有针对性的**长视野预测（预测里程碑）；而预测出的未来场景（短视野动态和长视野里程碑）为动作策略提供了丰富的上下文，从而生成更连贯、目标导向的动作。这构成了一个强大的内部反馈循环。\n\n### **四、 实际价值与意义**\n- **性能提升**：在R2R-CE和RxR-CE基准测试上达到了具有竞争力的性能（R2R-CE上SR 66.2%， OSR 78.4%），尤其在**Oracle成功率（OSR）** 上表现突出，证明了其预测机制能有效帮助智能体找到正确路径。\n- **泛化与效率**：模型仅使用公开数据集训练，展现了良好的泛化能力。通过预测高层特征而非像素，平衡了预测精度与计算效率。\n- **范式启发**：为具身智能领域提供了一个强有力的新范式——**将显式的语言推理与隐式的时空想象深度融合**。这更贴近人类“基于地标和预期进行分层导航”的认知方式，为开发更智能、更可靠的具身智能体开辟了道路。\n- **可解释性**：模型生成的层次化计划和双视野预测结果，为理解智能体的决策过程提供了直观的窗口，增强了系统的可解释性。",
      "timestamp": "2025-12-23T17:16:26.215100",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
      "answer": "## 论文分析：NavForesee及其所在领域的发展脉络\n\n基于对NavForesee论文的分析，该工作属于**视觉语言导航**领域，其核心创新在于**将高层次语言规划与预测性世界模型统一在一个框架内**。要梳理其发展脉络，我们需要从几个关键的发展方向来看，每一步的改进都旨在解决VLN（Vision-and-Language Navigation）中的核心挑战：**长时程规划、环境动态理解、以及指令与感知的语义对齐**。\n\n以下是该方向的主要发展脉络，按几个不同的技术范式演进：\n\n### 方向一：从反应式策略到分层与规划式系统\n这个方向关注如何让智能体从简单的“感知-动作”映射，升级为具备任务分解和前瞻性思考的能力。\n\n- **早期模型（基于图或地图的规划）**:\n    - **代表工作**: 基于**拓扑图**或**自上而下语义地图**的方法（如早期VLN模型）。\n    - **核心思想**: 将环境构建为图结构，节点代表位置，边代表可通行性。导航转化为在图上的路径搜索问题。\n    - **改进与局限**: 引入了显式的空间结构表示，但严重依赖精确的环境先验地图构建，在未知、连续环境中泛化能力差，且难以处理复杂的自然语言指令。\n\n- **VLM作为端到端策略**:\n    - **代表工作**: 直接使用大型视觉语言模型（VLM）作为策略网络，输入观测和指令，输出动作。\n    - **核心思想**: 利用VLM强大的多模态融合能力，实现从观察到动作的直接映射。\n    - **改进**: 大幅提升了指令理解和场景感知的语义能力，无需显式建图。\n    - **局限**: 本质上是**反应式**的，缺乏对长任务的分解和内部状态跟踪，容易在复杂、长程任务中“迷失”，且容易过拟合训练场景。\n\n- **VLM作为高层规划器**:\n    - **代表工作**: 使用VLM**自回归地生成动作计划或文本轨迹**（如一些CoT提示方法）。\n    - **核心思想**: 将VLM作为“慢思考”系统，进行逐步推理，生成分步计划，再交由底层控制器执行。\n    - **改进**: 引入了显式的、基于语言的推理链条，提升了任务分解和步骤规划的能力。\n    - **局限**: 推理链可能与环境动态脱节（产生“语义幻觉”），且逐步生成效率低，错误容易累积。\n\n- **双系统架构（Fast-and-Slow）**:\n    - **代表工作**: 将**深思熟虑的规划系统**与**轻量级反应式控制器**结合（如一些采用强化学习对齐的系统）。\n    - **核心思想**: 模仿人类双系统思维，慢系统负责高层规划，快系统负责快速执行和避障。\n    - **改进**: 平衡了推理的深度和执行的效率。\n    - **局限**: 规划与预测（世界模型）仍然是分离的模块，规划可能缺乏对物理环境动态的预见性。\n\n- **NavForesee的贡献（统一规划与预测）**:\n    - **核心思想**: **在一个统一的VLM框架内，同时进行分层语言规划和双时间尺度的世界模型预测**。\n    - **关键改进**:\n        1.  **内生闭环**: 将规划（产生子目标）与预测（想象达成子目标时的环境状态）紧密耦合。规划指导预测的目标，预测的结果为规划和行动提供丰富的上下文，形成“感知-规划/预测-行动”的内部反馈循环。\n        2.  **层次化规划**: 显式地跟踪任务进度（总结已完成子指令，明确当前子指令，规划下一步），使智能体的思考过程更结构化、可解释。\n        3.  **解决了“语义-动态”脱节问题**: 通过让同一个模型既懂语言又懂物理预测，避免了纯VLM规划的“语义幻觉”和纯世界模型的“语义漂移”。\n\n### 方向二：从无模型学习到世界模型与预测\n这个方向关注如何让智能体具备“想象力”，能够预测未来状态以更好地规划行动。\n\n- **经典世界模型（在潜在空间预测）**:\n    - **代表工作**: 在机器人学中，使用如RSSM等模型在紧凑的潜在空间中预测未来状态。\n    - **核心思想**: 学习环境的动态模型，用于在行动前通过“想象”来评估不同策略。\n    - **改进**: 为智能体提供了前瞻能力，使其不再是完全反应式的。\n    - **局限**: 通常是**动作条件**的，需要大量轨迹采样和评估，计算成本高；且**与高层语言指令和语义目标结合不紧密**。\n\n- **导航专用世界模型**:\n    - **代表工作**: `NavMorph` (RSSM用于导航)、`HNR` (预测多层次语义特征)。\n    - **核心思想**: 将世界模型概念应用于VLN任务，预测未来环境特征以优化策略。\n    - **改进**: `HNR`等工作提出预测高级语义特征（而非像素），提高了预测质量和效率，支持并行评估多个动作。\n    - **局限**: 仍然作为一个**独立的动态模型**存在，与负责理解指令和进行语义规划的VLM模块是分离的。这种分离导致规划与预测可能不一致。\n\n- **NavForesee的贡献（语言引导的预测）**:\n    - **核心思想**: **将世界模型的预测功能直接集成到VLM中，并由其语言规划模块进行引导**。\n    - **关键改进**:\n        1.  **双时间尺度预测**: 创新性地提出**短期预测**（用于局部避障和理解即时动态）和**长期预测**（用于想象下一个**语义里程碑**的状态）。这模仿了人类基于里程碑的导航策略。\n        2.  **特征级预测**: 延续了高效预测的思路，预测深度、DINOv2、SAM等高级特征，而非像素，平衡了信息量与计算开销。\n        3.  **结构化注意力机制**: 通过设计的注意力掩码，明确分离短/长期预测流，并让动作查询能够关注所有信息（历史、短/长期想象），确保了预测与行动决策的全局一致性。\n\n### 方向三：从监督学习到数据与训练范式创新\n这个方向关注如何获取高质量的训练数据以支持复杂的规划-预测任务。\n\n- **传统VLN数据集**:\n    - 如R2R, RxR，提供了指令-轨迹对，但缺乏任务分解和里程碑的标注。\n    - **局限**: 无法直接训练模型进行显式的层次化规划和里程碑预测。\n\n- **NavForesee的贡献（VLM驱动的数据生成）**:\n    - **核心方法**: 利用强大的VLM（Gemini 2.5 Pro）对原始VLN数据集（R2R-CE, RxR-CE）进行后处理，**自动生成层次化规划数据集**。\n    - **流程**: VLM分解长指令为子指令、识别关键帧里程碑、为路径上的航点生成“导航总结-未来计划-动作”标签。\n    - **关键改进**:\n        1.  **自动化标注**: 解决了人工标注层次化规划数据的巨大成本问题。\n        2.  **提供丰富监督信号**: 生成的数据同时包含了语言规划（子目标）和视觉预测（里程碑）所需的监督信息，使得**统一训练**规划与预测模型成为可能。\n\n### 总结与发展脉络图\n\n```mermaid\ngraph TD\n    A[早期VLN: 基于图/地图的规划] --> B[VLM作为端到端策略: 反应式， 语义强但无规划];\n    B --> C1[VLM作为规划器: 有推理链， 但可能脱离环境];\n    B --> C2[独立世界模型: 有预测， 但缺乏语言引导];\n    C1 & C2 --> D[双系统架构: 规划与执行分离];\n    D --> E[**NavForesee: 统一规划与预测**];\n\n    subgraph “NavForesee 的核心统一”\n        E --> F1[分层语言规划: 任务分解与进度跟踪];\n        E --> F2[双时间尺度世界模型: 短/长期特征预测];\n        F1 & F2 --> G[**内部感知-规划/预测-行动闭环**];\n    end\n```\n\n**脉络核心**：从 **“分离的模块化系统”** 走向 **“内聚的统一模型”**。NavForesee标志着一种范式转变，它不再将语言理解、任务规划和物理动态预测视为独立的子系统，而是试图在一个统一的、基于Transformer的架构中培养出同时具备这些能力的“全能”智能体。其实际价值在于为构建更**通用、鲁棒、类人**的具身导航智能体提供了可行的技术路径，通过“预见未来”来弥合高层语义意图与底层环境动态之间的鸿沟。",
      "timestamp": "2025-12-23T17:17:38.190506",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
      "answer": "## NavForesee 的前作分析与对比\n\n根据论文内容，NavForesee 的研究建立在视觉语言导航（VLN）和世界模型（World Model）两大领域的前期工作之上。论文在 **Related Works** 部分系统回顾了相关研究，并指出了现有方法的局限性。NavForesee 的核心创新在于**首次将 VLM 的高级语言规划与世界模型的时空预测能力统一在一个单一模型中**，形成了感知-规划/预测-行动的内部闭环。\n\n以下表格梳理了论文中明确提及或密切相关的主要前作，并详细对比了 NavForesee 与它们的核心区别和改进。\n\n### 前作梳理与对比分析\n\n| 前作类别 | 代表性工作 | 核心方法 / 特点 | 主要局限性 | NavForesee 的针对性改进与区别 |\n| :--- | :--- | :--- | :--- | :--- |\n| **VLM 作为规划器** | Instruc-Nav [31], AO-Planner [47] | 利用大语言模型（LLM）或 VLM 进行高级推理，生成逐步的动作计划或文本轨迹。 | 1. **规划与视觉现实脱节**：逐步生成的规划可能产生语义幻觉，与真实环境动态不符。<br>2. **错误累积**：自回归生成方式导致错误会逐步积累。<br>3. **推理效率低**：频繁的复杂推理（如长思维链）不必要且耗时。 | **统一规划与预测**：不再将规划与预测作为两个独立模块。NavForesee 的规划（生成子目标）直接指导其进行**有针对性的预测**（想象下一个里程碑的景象），而预测的结果又为行动提供丰富上下文，形成内部反馈循环，确保了规划与视觉现实的强关联。 |\n| **VLM 作为端到端策略** | 一些未具名的端到端 VLM 策略 | 将 VLM 作为策略网络，直接映射观测和指令到动作。 | 1. **过拟合**：容易过拟合于训练场景。<br>2. **未能充分利用高级推理**：将 VLM 当作“黑盒”控制器，未显式利用其分解任务、跟踪进度的能力。 | **引入显式分层语言规划**：NavForesee 明确让 VLM 扮演规划者角色，其核心功能之一是**分解长指令、跟踪任务进度、制定后续子目标**。这使高级语义推理得以显式化和结构化，而非隐含在策略中。 |\n| **双系统架构** | Nav-R1 [6], OmniNav [33] | 采用“快-慢”双系统：一个“慢”系统负责深思熟虑的规划，一个“快”系统负责反应式控制。常用强化学习对齐两者。 | **规划与动态环境对齐不足**：即使有“慢”系统推理，其生成的长推理链仍可能无法与环境的时空现实对齐。两个系统本质上是分离的，需要额外对齐。 | **单一模型内的紧密耦合**：NavForesee 在一个统一的 VLM（Qwen2.5-VL）内同时实现规划与预测。通过**结构化注意力掩码**，模型内部实现了短/长期预测查询与行动查询之间的可控信息流，使规划、预测、决策自然协同，无需复杂的多系统对齐。 |\n| **导航世界模型** | NavMorph [11], HNR [9], NWM [35] | 学习环境动态的预测模型，在潜在空间或像素空间预测未来状态，用于评估动作或规划轨迹。 | 1. **计算成本高**：基于动作采样的方法（如NWM）需要大量模拟，部署成本高。<br>2. **与语言推理割裂**：**这是最关键的局限**。现有世界模型专注于学习环境动力学，但**几乎完全忽略了与 VLM 高级语言推理能力的整合**。没有语言指导的预测会产生“语义漂移”，偏离指令目标。 | **语言引导的预测**：NavForesee 的世界模型预测**始终以完整的语言指令和历史观测为条件**。其“想象”的未来（无论是短期动态还是长期里程碑）是由当前的语言子目标所引导的，从根本上解决了语义漂移问题。**预测为规划服务**，规划为预测定向。 |\n| **特征预测世界模型** | HNR [9], DreamVLA [19] | 预测高级语义特征（如DINOv2, SAM特征），而非原始像素，以提高预测质量和速度。 | 虽然进行了特征预测，但其预测模块通常与语言规划模块是分离的，或者语言仅作为弱条件。 | **深度融合与双时间尺度预测**：NavForesee 继承了预测高级特征（深度、DINOv2、SAM）的思想以降低计算开销。其创新在于设计了**双时间尺度预测机制**：<br>- **短期预测**：固定步长，用于增强局部环境感知和避障。<br>- **长期预测**：自适应预测到**下一个语言子目标（里程碑）完成时**的环境特征，提供战略指导。<br>这种设计使预测既能辅助即时执行，又能服务于高层规划。 |\n\n### 总结：NavForesee 的核心改进\n\n1.  **范式统一**：**首创性地将 VLM 的显式语言规划与世界模型的隐式时空预测统一在一个模型框架内**，解决了前人工作中“规划脱离现实”和“预测脱离目标”的核心矛盾。\n2.  **架构创新**：基于统一的 VLM 主干（Qwen2.5-VL），通过引入**可学习的“梦想查询”** 和 **精心设计的结构化注意力掩码**，实现了规划、双时间尺度预测、行动决策在单一前向传播中的协同计算，结构更简洁高效。\n3.  **层次化与目标导向**：\n    - **规划层面**：提出了明确的分层语言规划范式，让模型学会像人类一样基于里程碑进行导航，而非连续的低级控制。\n    - **预测层面**：提出了**双时间尺度预测**，其中**长期预测直接与语言子目标（里程碑）挂钩**，使预测具有强烈的目标导向性，这是区别于以往仅预测固定步长未来状态的关键。\n4.  **数据与方法**：利用大模型（Gemini 2.5 Pro）自动化构建了大规模的**分层语言规划数据集**，为统一模型的训练提供了关键的数据基础，使模型能同时学习规划、预测和行动。\n\n**总而言之，NavForesee 不是对前作模块的简单堆砌，而是通过一个统一的、内聚的架构设计，实现了“语言规划”与“视觉想象”的深度融合与相互增强，从而在长视野、复杂指令的导航任务中实现了更鲁棒和高效的性能。**",
      "timestamp": "2025-12-23T17:18:32.437398",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
      "answer": "好的，作为一名专业的学术论文分析助手，我将仔细阅读并分析这篇题为《NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction》的论文，并详细阐述其提出的关键技术方法。\n\n## 论文核心技术创新分析\n\nNavForesee的核心创新在于**将高层次的语言规划与预测性的世界模型想象统一在一个单一的视觉-语言模型框架内**，形成了一个“感知-规划/预测-行动”的内部闭环。这解决了现有方法中规划与预测脱节的问题（如VLM规划脱离视觉现实，或世界模型预测脱离语言目标）。\n\n以下表格详细列出了其关键技术方法、细节、数学原理及参数：\n\n| 关键技术模块 | 核心思想与目标 | 具体技术细节与实现 | 数学原理与参数 |\n| :--- | :--- | :--- | :--- |\n| **1. 层次化语言规划** | 模仿人类导航，将长指令分解为子目标序列，并跟踪进度，为世界模型提供语义指导。 | **1.1 数据集构建：** <br>- **数据源：** 使用公开的R2R-CE (10k episodes) 和 RxR-CE (20k episodes) 数据集。<br>- **标注生成：** 利用 **Gemini 2.5 Pro** VLM，通过精心设计的提示模板，将长指令**自动分解**为一系列顺序的子指令，并识别对应的**关键帧里程碑**。<br>- **标签格式：** 对轨迹上的每个采样路径点，生成一个**规划标签**，包含：<br>  &nbsp;&nbsp;&nbsp;&nbsp;① **导航摘要**：已完成的子指令。<br>  &nbsp;&nbsp;&nbsp;&nbsp;② **未来计划**：下一个待执行的子指令。<br>  &nbsp;&nbsp;&nbsp;&nbsp;③ **语言动作**：`forward`, `left`, `right`, `stop`。<br>- **最终数据集：** 约150万训练样本（RxR-CE: 1.3M， R2R-CE: 0.2M），经过平衡采样和数据增强。<br><br>**1.2 模型训练：** <br>- **骨干网络：** 直接使用 **Qwen2.5-VL-3B-Instruct** 原有的多模态编码器和文本解码器。<br>- **训练方式：** 使用构建的规划数据集，以**自回归**的方式进行微调。模型学习根据完整指令和历史观测，生成结构化的规划输出（摘要、计划、动作）。 | **数学原理：** 标准的自回归语言建模。给定历史token序列 \\( x_{<t} \\)，模型最大化下一个token \\( x_t \\) 的条件概率：<br>\\[ P(x_t | x_{<t}, I, O_{t-H:t-1}) \\]<br>其中 \\( I \\) 是指令，\\( O \\) 是视觉观测序列。<br><br>**关键参数：** <br>- **骨干模型：** Qwen2.5-VL-3B-Instruct (30亿参数)。<br>- **历史长度 \\(H\\)：** 最大为20帧（灵活可变）。<br>- **输出：** 结构化的文本序列，遵循预定义的格式。 |\n| **2. 双视野世界模型预测** | 作为生成式世界模型，在**潜在特征空间**预测未来环境动态，为行动提供前瞻性信息。包含**短期**（固定步长）和**长期**（自适应里程碑）预测。 | **2.1 预测内容：** 避免像素级生成，预测紧凑的**高级特征**：<br>- **深度图**：几何信息。<br>- **DINOv2特征**：高级语义信息。<br>- **SAM特征**：实例级分割信息。<br><br>**2.2 模型架构扩展：** <br>- **位置编码器：** 新增模块，编码智能体从图像观测中得到的**相对位姿** \\( s_{t-H:t} \\)（位置和朝向）。<br>- **梦想查询：** 引入两组可学习的查询向量：<br>  &nbsp;&nbsp;&nbsp;&nbsp;• **短期查询** \\( Q_S \\in \\mathbb{R}^{L \\times d} \\) <br>  &nbsp;&nbsp;&nbsp;&nbsp;• **长期查询** \\( Q_L \\in \\mathbb{R}^{L \\times d} \\) <br>  每组查询又包含深度和语义子查询。\\( L=64 \\) 个token，\\( d \\) 为隐藏层维度。<br>- **结构化注意力掩码：** 设计专门的注意力模式以确保：<br>  &nbsp;&nbsp;&nbsp;&nbsp;① 长期预测以短期预测为条件（因果掩码）。<br>  &nbsp;&nbsp;&nbsp;&nbsp;② 深度与语义查询间**互不关注**，防止跨模态特征混合。<br>  &nbsp;&nbsp;&nbsp;&nbsp;③ **动作查询**可以关注所有信息（历史、短期、长期查询）。<br><br>**2.3 预测过程：** <br>1. 将指令嵌入 \\( l \\)、视觉观测序列 \\( O_{t-H:t} \\)、位姿嵌入 \\( h(s_{t-H:t}) \\) 和短期查询 \\( Q_S \\) 输入骨干网络 \\( f(\\cdot) \\)，得到短期嵌入 \\( E_S \\)。<br>2. 将 \\( E_S \\) 与长期查询 \\( Q_L \\) 一同输入，得到长期嵌入 \\( E_L \\)。<br>3. 通过**轻量级卷积解码器** \\( D(\\cdot) \\) 将嵌入解码为预测特征：<br>   - 短期预测：\\( p_{t+k} = D(E_S) = [d_p(t), c_p(t)] \\)<br>   - 长期预测：\\( p_{t+M_t} = D(E_L) = [d_p(t+M_t), c_p(t+M_t)] \\)<br>   其中 \\( M_t \\) 是到下一个里程碑的自适应步数。 | **数学原理：** <br>1. **特征预测：** 学习一个映射函数，从历史上下文和查询向量预测未来特征：<br>\\[ E_S = f(l, O_{t-H:t}, h(s_{t-H:t}); Q_S) \\]<br>\\[ E_L = f(l, O_{t-H:t}, h(s_{t-H:t}), E_S; Q_L) \\]<br>\\[ \\hat{p}_{t+\\Delta} = D(E_{S/L}) \\]<br><br>2. **损失函数（世界模型部分）：** <br>   - **深度损失 \\(L_d\\)**：尺度不变对数损失 (SiLogLoss)。<br>   - **语义特征损失 \\(L_c\\)**：均方误差 (MSE)。<br><br>**关键参数：** <br>- **查询维度：** \\( L=64 \\) tokens/查询类型，共6组子查询（深度、DINOv2、SAM × 短期、长期）。<br>- **预测视野：** 短期固定 \\( k=4 \\) 步；长期自适应 \\( M_t \\)。<br>- **损失权重：** \\( \\alpha = 0.25 \\) (深度)，\\( \\beta = 0.3 \\) (语义)。 |\n| **3. 预测性行动策略学习** | 基于历史观测、语言指令以及**世界模型预测的未来特征**，通过逆动力学原理学习连续的动作策略。 | **3.1 动作查询：** 引入一个可学习的**动作查询** \\( Q_a \\)。<br><br>**3.2 动作生成：** <br>1. 动作查询 \\( Q_a \\) 与梦想查询 \\( Q_S, Q_L \\) 及多模态输入一起，输入骨干网络，得到**动作嵌入** \\( E_a \\)。<br>2. **逆动力学模型** \\( M_{inv} \\) （一个MLP）以动作嵌入 \\( E_a \\) 和双视野预测特征 \\( E_S, E_L \\) 为条件，生成未来动作序列：<br>   \\[ \\hat{a}_{t:t+K} = M_{inv}(E_S, E_L | E_a) \\]<br>   其中每个动作是**连续路径点** \\( w_t = [x_t, y_t, \\sin\\theta_t, \\cos\\theta_t, c_t] \\)，\\( K=5 \\) 个路径点，\\( c_t \\) 是停止标志。<br><br>**3.3 训练：** 动作预测与特征预测**联合训练**。 | **数学原理：** 逆动力学学习。给定两个连续状态 \\( o_t \\) 和 \\( o_{t+1} \\)，推断中间动作 \\( a_t \\)：<br>\\[ \\hat{a}_t = g(o_t, o_{t+1}) \\approx M_{inv}(E_S, E_L, E_a) \\]<br>在NavForesee中，未来状态 \\( o_{t+1} \\) 由世界模型预测的特征 \\( E_S, E_L \\) 隐式提供。<br><br>**损失函数：** 动作损失 \\( L_a \\) ",
      "timestamp": "2025-12-23T17:19:44.317497",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
      "answer": "## 论文分析：NavForesee 的评价指标与数据集\n\n本文对提出的 NavForesee 模型在两个标准视觉语言导航（VLN）基准数据集上进行了全面评估，并使用了该领域一系列标准化的评价指标。\n\n### 一、 使用的数据集\n\nNavForesee 在两个主流的、具有挑战性的连续环境 VLN 数据集上进行训练和评估：\n\n| 数据集名称 | 来源/基础 | 规模与特点 | 任务难度与设置 |\n| :--- | :--- | :--- | :--- |\n| **R2R-CE** (Room-to-Room Continuous Environment) | 基于 Matterport3D 室内场景。 | 约 10k 条导航轨迹。提供**细粒度、逐步**的自然语言指令。 | **较高**。在 Habitat 模拟器中以**连续**方式执行导航（非离散网格）。<br>• **动作粒度**：可执行最小 `15°` 的转向。<br>• **视野**：`90°` 水平视场角。 |\n| **RxR-CE** (Room-across-Room Continuous Environment) | 大规模、多语言 VLN 数据集。 | 约 126k 条人类标注的指令，轨迹**更长、更复杂**，指令描述也更丰富。 | **非常高**。覆盖更多样和复杂的轨迹，增加了导航难度。<br>• **动作粒度**：更粗糙，最小转向增量为 `30°`。<br>• **视野**：更窄的 `79°` 水平视场角，要求更审慎的运动规划以有效覆盖场景。 |\n\n**关键细节**：\n*   **训练数据**：模型**仅**使用上述两个数据集的公开可用部分进行训练。论文通过 **Gemini 2.5 Pro** 对原始轨迹进行后处理，生成了用于分层规划的标注数据集（约150万样本）。\n*   **评估划分**：主要报告在 **`Val-Unseen`** 划分上的性能，这是衡量模型在**未见过的环境**中泛化能力的关键测试集。\n\n### 二、 使用的评价指标\n\n论文采用 VLN 领域公认的一组核心指标来全面衡量导航性能：\n\n| 指标名称 | 缩写 | 计算公式/定义 | 评价重点与细节 |\n| :--- | :--- | :--- | :--- |\n| **导航误差** | **NE** (Navigation Error) | 导航结束时，智能体位置与真实目标位置之间的**欧氏距离**（单位：米）。 | **衡量定位精度**。值**越低越好** (`↓`)。直接反映智能体最终是否靠近指令指定的目标点。 |\n| **成功率** | **SR** (Success Rate) | 如果导航终点与目标点的距离小于一个阈值（通常为3米），则视为成功。成功轨迹所占的百分比。 | **衡量任务完成度**。值**越高越好** (`↑`)。这是最核心的指标，表明智能体是否真正完成了指令。 |\n| **路径长度加权成功率** | **SPL** (Success weighted by Path Length) | `SPL = S * (L / max(P, L))` <br>其中：`S`（成功为1，否则为0），`L` 是最短路径长度，`P` 是智能体实际路径长度。 | **衡量导航效率**。值**越高越好** (`↑`)。它不仅要求成功，还惩罚那些绕远路才成功的轨迹，鼓励更直接、高效的路径。 |\n| **Oracle 成功率** | **OSR** (Oracle Success Rate) | 在智能体轨迹经过的所有位置点中，选择**离目标最近的那个点**，计算其成功率。 | **衡量模型“潜力”或感知能力**。值**越高越好** (`↑`)。它回答“如果智能体在最佳时刻停止，能否成功？”，排除了停止决策错误的影响，反映路径规划本身的质量。 |\n\n**指标使用细节**：\n1.  **综合评估**：这些指标通常被**同时报告**，以提供完整视图。例如，高 SR 但低 SPL 意味着智能体能到达目标但路径低效；高 OSR 但低 SR 意味着路径规划尚可，但停止决策模块是瓶颈。\n2.  **在本文中的应用**：\n    *   在 **R2R-CE** 上，NavForesee 取得了 **SR=66.2%**, **OSR=78.4%**, **NE=3.94m**, **SPL=59.7%** 的竞争性结果。\n    *   在 **RxR-CE** 上，性能为 **SR=66.3%**, **SPL=53.2%**, **NE=4.20m**。论文指出其在更复杂的 RxR-CE 上泛化性略逊于某些利用额外数据训练的 SOTA 模型，但 **OSR 在两者上均达到最高**，证明了其世界模型在路径想象方面的优势。\n3.  **消融实验的衡量标准**：在分析各模块（VLM规划、长/短期预测）贡献的消融研究中（表II），**SR 和 SPL 是判断性能下降的关键指标**，直接反映了模块移除对任务完成度和效率的影响。\n\n### 总结\n\nNavForesee 的工作严格遵循了 VLN 领域的标准实验范式：\n*   **数据集**：选用 **R2R-CE** 和 **RxR-CE** 这两个权威、公开的连续环境基准，确保了工作的可复现性和可比性。\n*   **评价指标**：采用 **NE, SR, SPL, OSR** 这套经典指标组合，从**准确性**（NE, SR）、**效率**（SPL）和**潜力分析**（OSR）多个维度提供了令人信服的性能证据。其优异的 **OSR** 尤其凸显了**融合语言规划与时空预测**这一核心创新的价值——即使有时停错了地方，但想象的路径本身常常是正确的。",
      "timestamp": "2025-12-23T17:20:32.745135",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
      "answer": "## 实验数据集与评估结果分析\n\n### 一、实验数据集\n论文在两个主流的视觉语言导航（VLN）连续环境基准数据集上进行了实验：\n\n1.  **R2R-CE** (Room-to-Room Continuous Environments)\n    *   **来源**：基于 Matterport3D 室内环境构建。\n    *   **特点**：提供细粒度、逐步的自然语言指令。在 Habitat 模拟器中，智能体可在连续环境中导航，支持最小 `15°` 的转向和 `90°` 的水平视野。\n    *   **数据量**：约 10k 条轨迹。\n\n2.  **RxR-CE** (Room-across-Room Continuous Environments)\n    *   **来源**：大规模、多语言 VLN 数据集。\n    *   **特点**：包含约 126K 条人工标注的指令，轨迹比 R2R-CE 更复杂、更多样。智能体转向增量为 `30°`，视野为 `79°`，对规划要求更高。\n    *   **数据量**：约 20k 条轨迹。\n\n**重要说明**：论文明确指出，NavForesee **仅使用**上述两个公开数据集的训练集进行训练，未使用其他大规模或私有数据来增强泛化能力。\n\n### 二、主要评估指标\n论文使用 VLN 领域的四个标准指标进行评估：\n\n1.  **导航误差 (Navigation Error, NE) ↓**：智能体停止位置与真实目标位置之间的欧氏距离（米）。值越低越好。\n2.  **成功率 (Success Rate, SR) ↑**：智能体最终位置距离目标在一定阈值（R2R-CE: 3米）内的轨迹比例。值越高越好。\n3.  **路径长度加权成功率 (Success weighted by Path Length, SPL) ↑**：在成功率的基础上，用智能体路径长度与最优路径长度的比值进行加权，同时衡量成功与效率。值越高越好。\n4.  **Oracle 成功率 (Oracle Success Rate, OSR) ↑**：在轨迹中任意点选择距离目标最近的位置作为停止点，计算其成功率。该指标衡量模型在轨迹中“最接近成功”的潜力，值越高越好。\n\n### 三、性能提升与对比分析\n论文在 **R2R-CE Val-Unseen** 和 **RxR-CE Val-Unseen** 分割上进行了主要性能对比。\n\n#### 1. 与现有方法的对比 (见表 I)\n*   **在 R2R-CE 上**：\n    *   NavForesee 取得了 **极具竞争力** 的性能。\n    *   **具体指标**：`SR = 66.2%`, `OSR = 78.4%`, `NE = 3.94m`, `SPL = 59.7%`。\n    *   **性能提升**：与基线方法相比，**SR 提升了 1.1%**，**OSR 显著提升了 10.9%**，**NE 降低了 0.3米**。这证明了其统一规划与预测框架的有效性。\n*   **在 RxR-CE 上**：\n    *   NavForesee 的性能 **略低于** 部分最先进（SOTA）方法。\n    *   **具体指标**：`SR = 66.3%`, `NE = 4.20m`, `SPL = 53.2%`。\n    *   **分析**：作者认为这归因于模型**仅使用公开数据集训练**，而其他SOTA方法可能利用了更多样化、大规模的数据来增强泛化到复杂环境的能力。\n*   **核心亮点**：尽管在RxR-CE上SR非最优，但NavForesee在**两个数据集上都取得了最高的OSR**。这表明模型具备强大的“接近成功”的潜力，其**世界模型的预测能力**帮助智能体在轨迹中做出了更接近目标的决策。\n\n#### 2. 消融实验分析 (见表 II)\n为了验证各模块的贡献，论文进行了系统的消融实验（在R2R-CE上）：\n\n| 编号 | VLM规划 | 长期预测 | 短期预测 | SR (%) | OSR (%) | NE (m) | SPL (%) |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| **1 (完整模型)** | **✓** | **✓** | **✓** | **66.2** | **78.4** | **3.94** | **59.7** |\n| 2 | ✗ | ✓ | ✓ | 48.8 | 75.5 | 5.61 | 39.4 |\n| 3 | ✓ | ✗ | ✓ | 58.6 | 76.4 | 4.47 | 50.1 |\n| 4 | ✗ | ✗ | ✗ | 52.6 | 67.4 | 5.53 | 46.7 |\n\n*   **VLM规划的重要性**：移除后（实验2），SR暴跌至48.8%，SPL下降超过16个百分点。这证明了**显式的指令分解和进度跟踪**对于高效导航至关重要。\n*   **长期预测的重要性**：移除后（实验3），SR下降至58.6%，NE升高。这表明**对里程碑的预见性**能为长轨迹提供战略指导。\n*   **模块协同作用**：完整模型（实验1）在所有指标上均最优，验证了**分层语言规划与双视野预测的紧密耦合**是提升长视野导航性能的关键。\n\n#### 3. 深度与语义预测的消融 (见表 III)\n论文还分析了世界模型中不同预测类型的作用：\n*   **同时使用深度和语义预测**（完整模型）效果最佳。\n*   **移除深度预测**：SR降至61.8%，SPL下降，说明深度信息对空间推理和避障很重要。\n*   **移除语义预测**：SR降至60.0%，影响更大，说明语义特征对识别地标和引导导航至关重要。\n\n### 总结\nNavForesee 在 **R2R-CE** 数据集上展现了 **SOTA 级别的性能**，特别是在 **OSR 指标上大幅领先**，验证了其将语言规划与世界模型预测相统一的核心创新价值。虽然在更复杂的 **RxR-CE** 数据集上泛化能力有待提升，但其框架设计为具身导航智能体提供了新的思路：**通过“预见未来”来更好地连接语言指令与时空现实**。",
      "timestamp": "2025-12-23T17:21:23.599431",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
      "answer": "## 论文主要局限性分析\n\n根据论文内容，NavForesee 存在以下主要局限性：\n\n- **泛化能力有限**：在更复杂、多样化的 RxR-CE 数据集上，性能略低于部分最先进方法。这表明模型对训练数据分布（R2R-CE 和 RxR-CE）存在依赖，在未见过的复杂环境或指令模式中泛化能力不足。\n- **预测精度与计算效率的权衡**：\n    - **深度预测较粗糙**：由于仅在 R2R-CE/RxR-CE 上进行像素级监督训练，生成的深度图较为粗糙，主要保留全局几何而非细节（见第 IV-C 节及图 3、6）。\n    - **长时预测不确定性**：长时里程碑预测在远离当前状态时准确性下降，尤其当里程碑位置未知时，预测可能退化为对短时预测的跟踪（见补充材料 II-B 节）。\n- **架构与训练约束**：\n    - **依赖特定 VLM 骨干**：基于 Qwen2.5-VL-3B-Instruct 构建，其性能上限和计算效率受该骨干模型制约。\n    - **训练数据规模相对有限**：仅使用公开的 R2R-CE/RxR-CE 数据集，而部分对比方法利用了更大规模、更多样的数据提升泛化能力（见第 IV-A 节）。\n- **未充分利用动态环境建模**：当前世界模型预测侧重于静态几何与语义特征，对环境中动态物体（如移动的人、门开关状态）的显式建模能力未充分探讨。\n\n## 未来可能的改进方向\n\n基于上述局限性，未来研究可从以下几个方向展开：\n\n- **增强数据多样性与规模**：\n    - 引入更大规模、多场景、多指令语言的导航数据集进行预训练或微调。\n    - 结合仿真环境自动生成多样化的指令-轨迹对，或利用更强大的 VLM（如 GPT-4V、Gemini）进行数据增强。\n- **提升预测模块的精度与效率**：\n    - 探索更精细的特征表示（如神经辐射场 NeRF、高斯溅射），在保持计算效率的同时提升几何与语义预测的保真度。\n    - 研究不确定性建模，使长时预测能自适应地反映置信度，避免错误传播。\n- **架构与算法优化**：\n    - 尝试更轻量或更强大的 VLM 骨干，并在预测模块中引入更高效的结构化注意力机制。\n    - 将动态物体预测纳入世界模型，使 agent 能应对更真实、非静态的环境。\n- **强化闭环与交互学习**：\n    - 引入在线学习或强化学习机制，使 agent 能在实际导航中根据失败经验持续优化规划与预测模块。\n    - 探索多模态反馈（如触觉、音频）的融合，进一步提升环境理解与决策鲁棒性。\n- **扩展任务范围**：\n    - 将框架推广到更具挑战性的任务，如交互式导航（需操作物体）、多 agent 协作导航或开放世界探索。\n\n**总结**：NavForesee 的创新在于**统一了语言规划与世界模型预测**，但其性能仍受数据、预测精度和泛化能力的限制。未来工作可通过**数据扩展、预测模块增强、动态环境建模及闭环学习**等方向，进一步提升 embodied agent 在复杂长时程任务中的鲁棒性与智能水平。",
      "timestamp": "2025-12-23T17:21:51.881221",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T11:24:16Z",
  "created_at": "2025-12-23T17:14:13.674785",
  "updated_at": "2025-12-23T17:14:13.674795"
}