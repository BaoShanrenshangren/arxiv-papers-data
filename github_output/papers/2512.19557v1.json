{
  "id": "2512.19557v1",
  "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
  "authors": [
    "Lawrence Krukrubo, Julius Odede, Olawande Olusegun"
  ],
  "abstract": "arXiv:2512.19557v1 Announce Type: new \nAbstract: Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
  "url": "https://arxiv.org/abs/2512.19557",
  "html_url": "https://arxiv.org/html/2512.19557v1",
  "html_content": "",
  "preview_text": "arXiv:2512.19557v1 Announce Type: new \nAbstract: Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "Explainable AI",
    "Hybrid Framework",
    "Scalability-Stability Dilemma",
    "Customer Churn Prediction",
    "Human-in-the-Loop AI"
  ],
  "one_line_summary": "该论文提出了一种混合框架来解决可解释AI的可扩展性与稳定性困境，应用于客户流失预测，减少人工标注工作量。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:47:47.617239",
  "updated_at": "2025-12-23T15:47:47.617249"
}