{
  "id": "2512.19069v1",
  "title": "Can abstract concepts from LLM improve SLM performance?",
  "authors": [
    "Siddharth Tandon"
  ],
  "abstract": "arXiv:2512.19069v1 Announce Type: new \nAbstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
  "url": "https://arxiv.org/abs/2512.19069",
  "html_url": "https://arxiv.org/html/2512.19069v1",
  "html_content": "",
  "preview_text": "arXiv:2512.19069v1 Announce Type: new \nAbstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "LLM",
    "SLM",
    "steering vectors",
    "inference-time scaling",
    "performance improvement"
  ],
  "one_line_summary": "该论文探讨了从大型语言模型提取抽象概念（如转向向量）以提升小型语言模型性能的方法，与关键词中的视频扩散或多模态生成等技术无关。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:44:41.892955",
  "updated_at": "2025-12-23T15:44:41.892965"
}