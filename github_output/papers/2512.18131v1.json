{
  "id": "2512.18131v1",
  "title": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation",
  "authors": [
    "Le Zhang, Suresh Kothari"
  ],
  "abstract": "arXiv:2512.18131v1 Announce Type: cross \nAbstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.",
  "url": "https://arxiv.org/abs/2512.18131",
  "html_url": "https://arxiv.org/html/2512.18131v1",
  "html_content": "",
  "preview_text": "arXiv:2512.18131v1 Announce Type: cross \nAbstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "LLMs",
    "code generation",
    "evaluation",
    "performance metrics",
    "prompt engineering"
  ],
  "one_line_summary": "该论文对六种最先进的大语言模型在代码生成任务上进行了全面评估，重点关注编译错误、运行时错误和算法优化等性能指标。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:49:00.876582",
  "updated_at": "2025-12-23T15:49:00.876599"
}