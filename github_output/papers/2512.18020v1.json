{
  "id": "2512.18020v1",
  "title": "Specification and Detection of LLM Code Smells",
  "authors": [
    "Brahim Mahmoudi, Zacharie Chenail-Larcher, Naouel Moha, Quentin Stievenert, Florent Avellaneda"
  ],
  "abstract": "arXiv:2512.18020v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.",
  "url": "https://arxiv.org/abs/2512.18020",
  "html_url": "https://arxiv.org/html/2512.18020v1",
  "html_content": "",
  "preview_text": "arXiv:2512.18020v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.",
  "is_relevant": false,
  "relevance_score": 0.0,
  "extracted_keywords": [
    "LLM code smells",
    "software quality",
    "detection tool",
    "inference practices"
  ],
  "one_line_summary": "这篇论文提出并检测了大型语言模型在软件系统中推理相关的代码异味，旨在提升软件质量。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:47:58.962483",
  "updated_at": "2025-12-23T15:47:58.962497"
}