{
  "id": "2512.18256v1",
  "title": "MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification",
  "authors": [
    "Sirui Li, Wangyue Lu, Xiaorui Shi, Ke Weng, Haozhe Sun, Minghe Yu, Tiancheng Zhang, Ge Yu, Hengyu Liu, Lun Du"
  ],
  "abstract": "arXiv:2512.18256v1 Announce Type: new \nAbstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.",
  "url": "https://arxiv.org/abs/2512.18256",
  "html_url": "https://arxiv.org/html/2512.18256v1",
  "html_content": "",
  "preview_text": "arXiv:2512.18256v1 Announce Type: new \nAbstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.",
  "is_relevant": false,
  "relevance_score": 0.0,
  "extracted_keywords": [
    "automated theorem proving",
    "mathematical reasoning",
    "benchmark",
    "formal verification",
    "large language model",
    "generalization",
    "coefficient of variation"
  ],
  "one_line_summary": "这篇论文提出了一个基于数学学科分类的自动定理证明基准MSC-180，用于评估大语言模型在数学推理中的泛化能力和领域覆盖问题。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "Tue, 23 Dec 2025 00:00:00 -0500",
  "created_at": "2025-12-23T15:44:31.011387",
  "updated_at": "2025-12-23T15:44:31.011401"
}